#!/usr/bin/python

#  Main authors:
#    Gabriel Hjort Blindell <ghb@kth.se>
#
#  Copyright (c) 2012-2017, Gabriel Hjort Blindell <ghb@kth.se>
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions are met:
#  1. Redistributions of source code must retain the above copyright notice,
#     this list of conditions and the following disclaimer.
#  2. Redistributions in binary form must reproduce the above copyright notice,
#     this list of conditions and the following disclaimer in the documentation
#     and/or other materials provided with the distribution.
#  3. Neither the name of the copyright holder nor the names of its contributors
#     may be used to endorse or promote products derived from this software
#     without specific prior written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY
#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
#  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



#=========
# IMPORTS
#=========

import json
import os.path
import sys
import time



#================
# HELP FUNCTIONS
#================

def error(msg):
    sys.stderr.write("ERROR: " + msg + "\n")
    sys.exit(1)



#=============
# MAIN SCRIPT
#=============

# Check arguments
if len(sys.argv) < 2:
    sys.stderr.write("No JSON file given\n")
    sys.exit(1)
if len(sys.argv) > 2:
    sys.stderr.write("Too many arguments\n")
    sys.exit(1)
json_file = sys.argv[1]
if not os.path.isfile(json_file):
    error("JSON file '" + json_file +"' not found")

time_start = time.time()

# Read JSON file
json_data = {}
with open(json_file, 'r') as file:
    json_data = json.load(file)

# Setup convenience variables
num_matches = json_data["num-matches"]
num_data = json_data["fun-num-data"]
num_operands = json_data["num-operands"]

# Convert lists of operand alternatives into sets
operand_alt_sets = [[]] * num_operands
for o in range(num_operands):
    operand_alt_sets[o] = set(json_data["operand-alternatives"][o])

# Compute set of data defined and set of data used per match
match_data_defined = [[]] * num_matches
match_data_used = [[]] * num_matches
for m in range(num_matches):
    match_data_defined[m] = [ d for d in range(num_data)
                                for o in json_data["match-operands-defined"][m]
                                    if d in operand_alt_sets[o]
                            ]
    match_data_used[m] = [ d for d in range(num_data)
                             for o in json_data["match-operands-used"][m]
                                 if d in operand_alt_sets[o]
                         ]

copy_matches = set(json_data["match-copy-instrs"])
null_matches = set(json_data["match-null-instrs"])

# Compute set of valid locations per datum from function
fun_data_locations = [None] * num_data
loc_data = json_data["fun-valid-value-locs"]
R = range(len(loc_data))
for i in range(len(R)):
    d = loc_data[i][0]
    l = loc_data[i][1]
    if fun_data_locations[d] is None:
        fun_data_locations[d] = set([l])
    else:
        fun_data_locations[d].add(l)

# Compute set of valid locations per datum per match
match_data_locations = []
for m in range(num_matches):
    match_data_locations.append([None] * num_data)
loc_data = json_data["match-valid-value-locs"]
R = range(len(loc_data))
for i in range(len(R)):
    m = loc_data[i][0]
    o = loc_data[i][1]
    l = loc_data[i][2]
    for d in operand_alt_sets[o]:
        if match_data_locations[m][d] is None:
            match_data_locations[m][d] = set([l])
        else:
            match_data_locations[m][d].add(l)

# Compute set of matches that define a particular datum
def_matches = [[]] * num_data
for d in range(num_data):
    def_matches[d] = { m for m in range(num_matches)
                             if d in match_data_defined[m]
                     }

# Compute set of matches that use a particular datum
use_matches = [[]] * num_data
for d in range(num_data):
    use_matches[d] = { m for m in range(num_matches)
                             if d in match_data_used[m]
                     }

# For each datum, compute the intersection of the locations of all of its defs
data_def_locs = [None] * num_data
for d in range(len(data_def_locs)):
    data_def_locs[d] = fun_data_locations[d]
    for m in def_matches[d]:
        if m in copy_matches:
            continue
        locs = match_data_locations[m][d]
        if locs is not None:
            if data_def_locs[d] is None:
                data_def_locs[d] = locs
            else:
                data_def_locs[d] = data_def_locs[d].intersection(locs)

# For each datum, compute the intersection of the locations of all of its uses
data_use_locs = [None] * num_data
for d in range(len(data_use_locs)):
    data_use_locs[d] = fun_data_locations[d]
    for m in use_matches[d]:
        if m in copy_matches:
            continue
        locs = match_data_locations[m][d]
        if locs is not None:
            if data_use_locs[d] is None:
                data_use_locs[d] = locs
            else:
                data_use_locs[d] = data_use_locs[d].intersection(locs)

# Propagate location information involved in phi operations
locs_updated = True
while locs_updated:
    locs_updated = False
    for m in json_data["match-phi-instrs"]:
        d_defs = match_data_defined[m]
        d_uses = match_data_used[m]

        new_locs = None

        # Compute intersection of all use and def locations
        for d in d_defs:
            locs = data_def_locs[d]
            if locs is not None:
                if new_locs is None:
                    new_locs = locs
                else:
                    new_locs = new_locs.intersection(locs)

        for d in d_uses:
            locs = data_use_locs[d]
            if locs is not None:
                if new_locs is None:
                    new_locs = locs
                else:
                    new_locs = new_locs.intersection(locs)

        # Update use and def location information
        for d in d_defs:
            if data_def_locs[d] != new_locs:
                data_def_locs[d] = new_locs
                locs_updated = True
        for d in d_uses:
            if data_use_locs[d] != new_locs:
                data_use_locs[d] = new_locs
                locs_updated = True

# Find the data that belong phi operations
phi_data = set()
for m in json_data["match-phi-instrs"]:
    d_defs = match_data_defined[m]
    d_uses = match_data_used[m]
    phi_data = phi_data.union(set(d_defs))
    phi_data = phi_data.union(set(d_uses))

redundant_matches = set()

kill_matches = set(json_data["match-kill-instrs"])

# For each copy operation o, check if there is a null copy match to cover it. If
# so, then the kill match covering o is redundant
null_copy_ops = set()
for m in copy_matches & null_matches - kill_matches:
    null_copy_ops = \
        null_copy_ops | set(json_data["match-operations-covered"][m])
for m in kill_matches:
    os = set(json_data["match-operations-covered"][m])
    if os <= null_copy_ops:
        redundant_matches.add(m)

# For each non-null copy match, check if there is (full or partial) overlap
# between the datum it uses and the datum it defines. If there is, then the
# match is redundant
for m in copy_matches - null_matches:
    use_d = match_data_used[m][0]
    def_d = match_data_defined[m][0]

    # Copies which load constants are never redundant
    if use_d in json_data["fun-const-data"]:
        continue

    # Skip if any of the data belongs to a phi
    if def_d in phi_data:
        continue

    use_locs = data_def_locs[use_d]
    def_locs = data_use_locs[def_d]
    if ( use_locs is None or
         def_locs is None or
         len(use_locs & def_locs) > 0
       ):
        redundant_matches.add(m)

time_elapsed = time.time() - time_start

# Print JSON
r_json_data = {}
r_json_data["redundant-null-matches-ai"] = list(redundant_matches)
r_json_data["processing-time"] = time_elapsed
print json.dumps(r_json_data)
